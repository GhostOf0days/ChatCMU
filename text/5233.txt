To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes.Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices.Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements.At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum.The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for It would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research.Home

Team
TeamDevelopment Timeline

Week 15 Week 14 Week 13 Week 12 Week 11 Week 10 Week 09 Week 08 Week 07 Week 06 Week 05 Week 04 Week 03 Week 02 Week 01
Week 15Week 14Week 13Week 12Week 11Week 10Week 09Week 08Week 07Week 06Week 05Week 04Week 03Week 02Week 01Design Documentation

Introduction Prototype 1a – 2D Geometry Prototype 1b – 3D Line Prototype 2 – Biomes Prototype 3 – Chemistry Identified Design Patterns Conclusion
IntroductionPrototype 1a – 2D GeometryPrototype 1b – 3D LinePrototype 2 – BiomesPrototype 3 – ChemistryIdentified Design PatternsConclusionHome

Team

TeamDevelopment Timeline

Week 15
Week 14
Week 13
Week 12
Week 11
Week 10
Week 09
Week 08
Week 07
Week 06
Week 05
Week 04
Week 03
Week 02
Week 01

Week 15Week 14Week 13Week 12Week 11Week 10Week 09Week 08Week 07Week 06Week 05Week 04Week 03Week 02Week 01Design Documentation

Introduction
Prototype 1a – 2D Geometry
Prototype 1b – 3D Line
Prototype 2 – Biomes
Prototype 3 – Chemistry
Identified Design Patterns
Conclusion

IntroductionPrototype 1a – 2D GeometryPrototype 1b – 3D LinePrototype 2 – BiomesPrototype 3 – ChemistryIdentified Design PatternsConclusionA quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to use
Skip to content

















Home

Team
Development Timeline

Week 15 Week 14 Week 13 Week 12 Week 11 Week 10 Week 09 Week 08 Week 07 Week 06 Week 05 Week 04 Week 03 Week 02 Week 01
Design Documentation

Introduction Prototype 1a – 2D Geometry Prototype 1b – 3D Line Prototype 2 – Biomes Prototype 3 – Chemistry Identified Design Patterns Conclusion






Menu
Close

















Home

Team


Development Timeline

Week 15
Week 14
Week 13
Week 12
Week 11
Week 10
Week 09
Week 08
Week 07
Week 06
Week 05
Week 04
Week 03
Week 02
Week 01


Design Documentation

Introduction
Prototype 1a – 2D Geometry
Prototype 1b – 3D Line
Prototype 2 – Biomes
Prototype 3 – Chemistry
Identified Design Patterns
Conclusion







 



















Prototype 1a - 2D Geometry Demo 










Objective 



To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 










Platform 



Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  



 










Early Design 



 










Development Process 



I. HoloLens 



We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 



II. Pass-through VR 



Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 



 



At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 







 







 














Results 



The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 



 



















































































Home

Team
Development Timeline

Week 15 Week 14 Week 13 Week 12 Week 11 Week 10 Week 09 Week 08 Week 07 Week 06 Week 05 Week 04 Week 03 Week 02 Week 01
Design Documentation

Introduction Prototype 1a – 2D Geometry Prototype 1b – 3D Line Prototype 2 – Biomes Prototype 3 – Chemistry Identified Design Patterns Conclusion






Menu
Close

















Home

Team


Development Timeline

Week 15
Week 14
Week 13
Week 12
Week 11
Week 10
Week 09
Week 08
Week 07
Week 06
Week 05
Week 04
Week 03
Week 02
Week 01


Design Documentation

Introduction
Prototype 1a – 2D Geometry
Prototype 1b – 3D Line
Prototype 2 – Biomes
Prototype 3 – Chemistry
Identified Design Patterns
Conclusion







 



















Prototype 1a - 2D Geometry Demo 










Objective 



To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 










Platform 



Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  



 










Early Design 



 










Development Process 



I. HoloLens 



We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 



II. Pass-through VR 



Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 



 



At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 







 







 














Results 



The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 



 



























































































Home

Team
Development Timeline

Week 15 Week 14 Week 13 Week 12 Week 11 Week 10 Week 09 Week 08 Week 07 Week 06 Week 05 Week 04 Week 03 Week 02 Week 01
Design Documentation

Introduction Prototype 1a – 2D Geometry Prototype 1b – 3D Line Prototype 2 – Biomes Prototype 3 – Chemistry Identified Design Patterns Conclusion






Menu
Close

















Home

Team


Development Timeline

Week 15
Week 14
Week 13
Week 12
Week 11
Week 10
Week 09
Week 08
Week 07
Week 06
Week 05
Week 04
Week 03
Week 02
Week 01


Design Documentation

Introduction
Prototype 1a – 2D Geometry
Prototype 1b – 3D Line
Prototype 2 – Biomes
Prototype 3 – Chemistry
Identified Design Patterns
Conclusion







 









Home

Team
Development Timeline

Week 15 Week 14 Week 13 Week 12 Week 11 Week 10 Week 09 Week 08 Week 07 Week 06 Week 05 Week 04 Week 03 Week 02 Week 01
Design Documentation

Introduction Prototype 1a – 2D Geometry Prototype 1b – 3D Line Prototype 2 – Biomes Prototype 3 – Chemistry Identified Design Patterns Conclusion






Menu
Close


















Home

Team
Development Timeline

Week 15 Week 14 Week 13 Week 12 Week 11 Week 10 Week 09 Week 08 Week 07 Week 06 Week 05 Week 04 Week 03 Week 02 Week 01
Design Documentation

Introduction Prototype 1a – 2D Geometry Prototype 1b – 3D Line Prototype 2 – Biomes Prototype 3 – Chemistry Identified Design Patterns Conclusion






Menu
Close






Home

Team
Development Timeline

Week 15 Week 14 Week 13 Week 12 Week 11 Week 10 Week 09 Week 08 Week 07 Week 06 Week 05 Week 04 Week 03 Week 02 Week 01
Design Documentation

Introduction Prototype 1a – 2D Geometry Prototype 1b – 3D Line Prototype 2 – Biomes Prototype 3 – Chemistry Identified Design Patterns Conclusion






Menu
Close




Home

Team
Development Timeline

Week 15 Week 14 Week 13 Week 12 Week 11 Week 10 Week 09 Week 08 Week 07 Week 06 Week 05 Week 04 Week 03 Week 02 Week 01
Design Documentation

Introduction Prototype 1a – 2D Geometry Prototype 1b – 3D Line Prototype 2 – Biomes Prototype 3 – Chemistry Identified Design Patterns Conclusion





Menu
Close























Home

Team


Development Timeline

Week 15
Week 14
Week 13
Week 12
Week 11
Week 10
Week 09
Week 08
Week 07
Week 06
Week 05
Week 04
Week 03
Week 02
Week 01


Design Documentation

Introduction
Prototype 1a – 2D Geometry
Prototype 1b – 3D Line
Prototype 2 – Biomes
Prototype 3 – Chemistry
Identified Design Patterns
Conclusion







 







 













Prototype 1a - 2D Geometry Demo 










Objective 



To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 










Platform 



Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  



 










Early Design 



 










Development Process 



I. HoloLens 



We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 



II. Pass-through VR 



Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 



 



At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 







 







 














Results 



The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 



 


























































Prototype 1a - 2D Geometry Demo 










Objective 



To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 










Platform 



Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  



 










Early Design 



 










Development Process 



I. HoloLens 



We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 



II. Pass-through VR 



Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 



 



At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 







 







 














Results 



The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 



 
























































Prototype 1a - 2D Geometry Demo 










Objective 



To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 










Platform 



Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  



 










Early Design 



 










Development Process 



I. HoloLens 



We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 



II. Pass-through VR 



Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 



 



At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 







 







 














Results 



The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 



 





















































Prototype 1a - 2D Geometry Demo 










Objective 



To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 










Platform 



Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  



 










Early Design 



 










Development Process 



I. HoloLens 



We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 



II. Pass-through VR 



Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 



 



At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 







 







 














Results 



The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 



 


















































Prototype 1a - 2D Geometry Demo 










Objective 



To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 










Platform 



Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  



 










Early Design 



 










Development Process 



I. HoloLens 



We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 



II. Pass-through VR 



Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 



 



At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 







 







 














Results 



The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 



 















































Prototype 1a - 2D Geometry Demo 










Objective 



To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 










Platform 



Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  



 










Early Design 



 










Development Process 



I. HoloLens 



We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 



II. Pass-through VR 



Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 



 



At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 







 







 














Results 



The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 



 












































Prototype 1a - 2D Geometry Demo 










Objective 



To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 










Platform 



Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  



 










Early Design 



 










Development Process 



I. HoloLens 



We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 



II. Pass-through VR 



Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 



 



At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 







 







 














Results 



The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 



 










































Prototype 1a - 2D Geometry Demo 










Objective 



To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 










Platform 



Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  



 










Early Design 



 










Development Process 



I. HoloLens 



We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 



II. Pass-through VR 



Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 



 



At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 







 







 














Results 



The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 



 








































Prototype 1a - 2D Geometry Demo 

Prototype 1a - 2D Geometry Demo 













Objective 

Objective 

To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 

To create an interactive model of AR marker vertices that demonstrate properties of 2D shapes. 













Platform 

Platform 

Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  

Originally Microsoft HoloLens, shifted to Oculus Rift + Zed mini + Leap  

 

 













Early Design 

Early Design 

 

 













Development Process 

Development Process 

I. HoloLens 

I. HoloLens 

We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 

We started building this demo with Microsoft HoloLens. HoloLens provides three types of interaction: Tap, Bloom and Gaze. Bloom is reserved for system use, however, and therefore we tried to build interactions using Tap and Gaze.The first prototype features a triangle with three movable vertices. Moving vertices requires the guest to first tap on it to enable movement, use gazing to drag the vertex and finally tap on the re-positioned vertex to switch it back to locked-in position. We wanted to see if it might feel fluid in action, but due to gesture recognition issues, the interactions were not smooth and felt cumbersome. This eventually led to a platform shift as discussed in the Introduction.When we tried to instantiate polygons with more than three sides, we found that spawning vertices based on individual markers resulted in vertices not sharing the same plane. To get around this, each vertex spawned after the third one was anchored to the plane determined by the first three vertices. 

II. Pass-through VR 

II. Pass-through VR 

Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 

Using the Leap Motion for interactions meant that the vertices could be dragged and dropped anywhere in space using natural hand movements. 

 

 

At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 

At this point, we investigated the issue of image tracking with this new platform. Directing efforts to the image tracking R&D reduced the scope of this prototype to just implementing a few basic features, independent of the interactive marker feature.A primary need is to display angle measurements as the user moves the vertices. The Leap Motion SDK has a responsive flip panel that can appear when the user flips their palm. We used that panel to display the internal angle measures and their constant sum. 




 







 







 





 



 

 



 





 



 

 













Results 

Results 

The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 

The palm flip interaction itself is engaging, but in its current state, it requires that the users hold their hand up for an extended period of time. Also, users are forced to interact one-handed for the duration that they want the panel displayed.A panel like this may instead be best suited for A quick check on data, akin to checking a wristwatch for timeAn inventory, from where you can pull out objects to useIt would also be preferable to have the angle measures overlaid directly on the polygon for a more direct correlation.One positive implication of this demo is the role of embodied cognition in math education. Previous research has implied that physical embodiment of mathematical concepts could be a powerful tool for developing mental models. Experiences in this vein might indicate how AR can further that research. 

 

 



























































































































































